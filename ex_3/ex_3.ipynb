{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Variational Autoencoders\n",
    "---\n",
    "![VAE-cover](images/vae_cover.png)\n",
    "<!-- ## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [MNIST](#part-i-mnist)\n",
    "    - [](#tool-pytorch-lightning) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this exercise, you will build and apply variational autoencoders to the [MNIST dataset](https://yann.lecun.com/exdb/mnist/) (hand-written digits). While VAEs may no longer top the race in terms of generation quality compared to newer frameworks such as diffusion models, they remain a critical tool in various applications. VAEs are not only useful for data generation but also for tasks like anomaly detection, semi-supervised learning, and feature selection. In reinforcement learning, for instance, VAEs are employed to learn compact, informative representations of the environment, which can simplify state-space representations and improve policy learning. They also play a crucial role in disentangling latent features, aiding in interpretable and controllable generative processes.\n",
    "\n",
    "> MNIST has been used for many different projects, from handwritting recognition tasks to generative AI (such as this one!). Check out the [link](https://yann.lecun.com/exdb/mnist/) to see some of these projects, and maybe even try a couple yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image, make_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the dataset and training parameters:\n",
    "\n",
    "Before building the model, we need to define some basic parameters and understand the shape of our data. MNIST images are ***28x28 pixels*** and since they are greyscale, ***they only have a single channel***. \n",
    "\n",
    "We also need to decide the size of the ***latent space*** - This is the dimensionality of the vector where the encoder compresses the input. A common choice is ***20 dimensions***, but you can experiment with smaller or larger latent spaces to see how it affects the quality of the generated images. This is considered the ***bottleneck*** of the VAE - The compressed representation of the data. ***Too small*** and the model cannot capture the variations in the data, reconstruction will lose details. ***too large*** and the latent space may become under-regularized, leading to poor generative properties (e.g., sampling from the latent space produces meaningless images).\n",
    "\n",
    "> Practical rule for MNIST: 10-50 dimensions often work well (hence why we reocmmend first trying with a value of 20 and then experimenting). Try a range of values (even if it comes out as expected first try) to see what different latent space dimensions does to a VAE.\n",
    "\n",
    "You will also want to define other important parameters such as ***batch size***, ***learning rate***, ***hidden dimensions*** and ***epochs***.\n",
    "\n",
    "<strong style=\"color:red;\">TODO 1.1: Set the hyperparameters for your model:</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '~/datasets'\n",
    "cuda = False\n",
    "DEVICE = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# TODO 1.1: Set hyperparameters\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "batch_size = ...\n",
    "\n",
    "x_dim = ...\n",
    "hidden_dim = ...\n",
    "latent_dim = ...\n",
    "\n",
    "lr = ...\n",
    "\n",
    "epochs = ...\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# END TODO 1.1\n",
    "# ---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data:\n",
    "\n",
    "Since the MNIST dataset is so widely used, it has become one of the many `torchvision` datasets which can be loaded as a package. For this reason, we use the available packages to load our train and test datasets/loaders. This is code we provide for you, however you can also find documentation on other `torchvision.datasets` if you want to try out other cool projects using online data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "mnist_transform = transforms.Compose([transforms.ToTensor(), ])\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "\n",
    "train_dataset = MNIST(dataset_path, train=True, download=True, transform=mnist_transform)\n",
    "test_dataset = MNIST(dataset_path, train=False, download=True, transform=mnist_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the encoder-decoder structure in a VAE\n",
    "\n",
    "---\n",
    "\n",
    "![VAE Diagram](./images/vae_diagram.jpg)\n",
    "\n",
    "(this image comes from [pyimagesearch](https://pyimagesearch.com/2023/10/02/a-deep-dive-into-variational-autoencoders-with-pytorch/) A full explanation for how a vae works in depth can be found there too!)\n",
    "\n",
    "### Encoder: Mapping inputs to a dsitribution\n",
    "\n",
    "The encoder's role is to compress the input image into a representation in the latent space. But unlike a standard autoencoder, the VAE encoder outputs a **distribution**, not a single vector. For each input ***x***, the encoder produces two vectors:\n",
    "- **mean vector** ($\\mu$) - the center of the latent distribution\n",
    "- **Log-variance vector** (log $\\sigma^2$) - describes how spread out the distribution is.\n",
    "\n",
    "Formally:\n",
    "\n",
    "$(\\mu, log(\\sigma^2))$ = Encoder($x$)\n",
    "\n",
    "This probabalistic encoding allows the latent space to capture both **what features are important** and **how uncertain the model is** about them.\n",
    "\n",
    "<strong style=\"color:red;\">TODO 2.1: Define the Encoder:</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# TODO 2.1: Implement the Encoder\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        pass\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "        return z_mean, z_log_var\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# END TODO 2.1\n",
    "# ----------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder: Reconstructing the input\n",
    "\n",
    "The decoder takes a point from the latent space and attempts to reconstruct the original image.\n",
    "\n",
    "$\\hat x$ = Decoder($z$)\n",
    "\n",
    "Here $\\hat x$ is the reconstructed image, and its similarity to the original input $x$ is measured using the **reconstruction loss**. If the encoder has captures the right features, the decoder can recreate images that look very much like the originals.\n",
    "\n",
    "<strong style=\"color:red;\">TODO 2.2: Define the Decoder:</strong>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# TODO 2.2: Implement the Decoder\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "        return x_hat\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# END TODO 2.2\n",
    "# ----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it together: the reparameterization trick\n",
    "\n",
    "To connect the encoder and decoder, we need to sample a latent vector $z$ from the distribution produced by the encoder. However, naive sampling would break backpropogation. The solution sis the **reparameterization trick**:\n",
    "\n",
    "$z = \\mu + \\sigma \\odot \\epsilon$\n",
    "\n",
    "$\\epsilon ~ \\mathcal{N}(0, I)$\n",
    "\n",
    "where: \n",
    "- $\\mu$ is the mean from the encoder\n",
    "- $\\sigma$ = $exp(0.5 * log (\\sigma^2))$ is the standard deviation\n",
    "- $\\epsilon$ is random noise drawn from a standard normal distribution\n",
    "- $\\odot$ denotes element-wise multiplication.\n",
    "\n",
    "This formulation keeps the randomness while allowing gradients to flow, making the model trainable. By combining **probabalistic encoding**, **differentiable sapling**, and **decoding**, the VAE learns a **smooth and contineous latent space**. This not only enables faithful reconstruction of digits but also makes it possible to generate **entirely new samples** by drawing random vectors from the latent space.\n",
    "\n",
    "<strong style=\"color:red;\">TODO 2.3: Implement the full VAE:</strong>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# TODO 2.3: Implement the VAE\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, Encoder, Decoder):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Define the encoder and decoder\n",
    "        pass\n",
    "\n",
    "    def reparameterization(self, mean, var):\n",
    "        pass\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "        return x_hat, mean, log_var\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# END TODO 2.3\n",
    "# ----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)\n",
    "decoder = Decoder(latent_dim=latent_dim, hidden_dim=hidden_dim, output_dim=x_dim)\n",
    "\n",
    "model = VAE(Encoder=encoder, Decoder=decoder).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the optimizer and loss function\n",
    "\n",
    "Now that the model is defined, we need two more ingredients before we can start training: \n",
    "1. A **loss function** that tells the model how well its doing.\n",
    "2. An **optimizer** that updates the model's parameters based on that loss.\n",
    "\n",
    "---\n",
    "\n",
    "### The VAE loss function\n",
    "\n",
    "The loss in the the VAE has two parts: \n",
    "\n",
    "1. **Reconstruction loss**:\n",
    "    - Measures how close the reconstructed imahe $\\hat x$ is to the original input image $x$.\n",
    "    - Here, we use **binary cross-entropy (BCE)**, summed over all pixels.\n",
    "    - Essentially the model gets penalized if it can't recreate the input digits correctly.\n",
    "\n",
    "    $\\mathrm{recon} = \\mathrm{BCE}(x, \\hat x)$\n",
    "\n",
    "2. **KL divergence loss**:\n",
    "    - Regularizes the latent space so that the encoded distribution $\\mathcal{N}(\\mu, \\sigma^2)$ stay close to a standard normal distribution $\\mathcal{N}(0, I)$. \n",
    "    - This keeps the latent space smooth and ensures that sampling random points produces meaningful digits\n",
    "\n",
    "    $\\mathrm{KL} = - 0.5 \\sum(1 + log(\\sigma^2) - \\mu^2, - \\sigma^2)$\n",
    "    \n",
    "    The final loss is the sum of both terms (with an optional scaling factor $\\beta$ to control the strength of the KL term):\n",
    "\n",
    "    $\\mathcal{L} = \\mathrm{recon} + \\beta * KL$\n",
    "\n",
    "    Dividing by the batch size helps keep th eloss values stable.\n",
    "\n",
    "### The optimizer:\n",
    "\n",
    "To train the model we use the **Adam optimizer**. Adam adapts the learning rate for each parameter, making training faster and more stable than just standard gradient descent. Therefore to make it run we pass in the `model parameters` and the `learning rate`.\n",
    "\n",
    "<strong style=\"color:red;\">TODO 3.1-3.2: Define the loss function and the optimizer:</strong>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# TODO 3.1: Implement the loss function\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(x, x_hat, mean, log_var, beta=1.0):\n",
    "    # reconstruction loss (BCE summed over pixels)\n",
    "    pass\n",
    "\n",
    "    # KL divergence term\n",
    "    pass\n",
    "\n",
    "    # normalize by batch size for stability\n",
    "    return (recon + beta * kl) / x.size(0)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# END TODO 3.1\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# TODO 3.2: Set up the optimizer\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "optimizer = pass\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# END TODO 3.2\n",
    "# ----------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The training loop:\n",
    "\n",
    "With the model, loss, and optimizer defined, we can now put everything together in a training loop. Training a VAE looks similar to training other neural networks, but we track three key values:  \n",
    "\n",
    "1. **Reconstruction loss** – measures how well the decoder reproduces the input images.  \n",
    "2. **KL divergence** – regularizes the latent space so it stays close to a standard normal distribution.  \n",
    "3. **Total loss** – the sum of the two, which the optimizer minimizes.  \n",
    "\n",
    "At each epoch:  \n",
    "- We loop through the training batches and pass the images through the encoder and decoder.  \n",
    "- We compute the reconstruction and KL terms separately to better monitor how the model is learning.  \n",
    "- The gradients are reset (`optimizer.zero_grad()`), the loss is backpropagated (`loss.backward()`), and the optimizer updates the model’s parameters (`optimizer.step()`).  \n",
    "\n",
    "Using `tqdm`, we also show a progress bar with the current loss values for each batch, making it easier to see improvements during training. At the end of each epoch, we print the average losses so we can track the overall learning progress.  \n",
    "\n",
    "By monitoring both reconstruction and KL divergence, we ensure the model is **balancing accurate reconstructions with a well-structured latent space**, which is the essence of training a VAE.  \n",
    "\n",
    "<strong style=\"color:red;\">TODO 4.1: Write and run the training loop:</strong>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Start training VAE...\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# TODO 4.1: Implement the training loop\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    pass\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# END TODO 4.1\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print(\"Finish!!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result visualization:\n",
    "\n",
    "We provide a simple overview of your results. The top row shows the input image and the bottom row shows the images the network reconstructs. Feel free to try create other meaningful visualisations, as it tests your ability to check the performance of your models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from additional.plots import plot_reconstructions\n",
    "\n",
    "plot_reconstructions(model, test_loader, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '~/datasets'\n",
    "cuda = False\n",
    "DEVICE = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# TODO 1.1: Set hyperparameters\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "x_dim = 784\n",
    "hidden_dim = 256\n",
    "latent_dim = 10\n",
    "\n",
    "lr = 1e-3\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# END TODO 1.1\n",
    "# ---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING MODEL:\n",
    "\n",
    "\"\"\"\n",
    "simple Gaussian MLP Encoder and Decoder\n",
    "\"\"\"\n",
    "# ----------------------------------------------------------------------------\n",
    "# TODO 2.1: Implement the Encoder\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.fc_input = nn.Linear(x_dim, hidden_dim)\n",
    "        self.fc_input2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc2_mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc2_log_var = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        self.ReLU = nn.ReLU(0.2)\n",
    "\n",
    "        self.training = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.ReLU(self.fc_input(x))\n",
    "        h = self.ReLU(self.fc_input2(h))\n",
    "        z_mean = self.fc2_mean(h)\n",
    "        z_log_var = self.fc2_log_var(h)\n",
    "        return z_mean, z_log_var\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# END TODO 2.1\n",
    "# ----------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# TODO 2.2: Implement the Decoder\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.fc_hidden = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc_hidden2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_output = nn.Linear(hidden_dim, x_dim)\n",
    "\n",
    "        self.ReLU= nn.ReLU(0.2)\n",
    "\n",
    "        self.training = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.ReLU(self.fc_hidden(x))\n",
    "        h = self.ReLU(self.fc_hidden2(h))\n",
    "\n",
    "        x_hat = torch.sigmoid(self.fc_output(h))\n",
    "        return x_hat\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# END TODO 2.2\n",
    "# ----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# TODO 2.3: Implement the VAE\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, Encoder, Decoder):\n",
    "        super(VAE, self).__init__()\n",
    "        self.Encoder = Encoder\n",
    "        self.Decoder = Decoder\n",
    "\n",
    "    def reparameterization(self, mean, var):\n",
    "        epsilon = torch.randn_like(var).to(DEVICE)\n",
    "        z = mean + var * epsilon\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, log_var = self.Encoder(x)\n",
    "        z = self.reparameterization(mean, torch.exp(0.5 * log_var))\n",
    "        x_hat = self.Decoder(z)\n",
    "\n",
    "        return x_hat, mean, log_var\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# END TODO 2.3\n",
    "# ----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# TODO 3.1: Implement the loss function\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(x, x_hat, mean, log_var, beta=1.0):\n",
    "    # reconstruction loss (BCE summed over pixels)\n",
    "    recon = F.binary_cross_entropy(x_hat, x, reduction=\"sum\")\n",
    "\n",
    "    # KL divergence term\n",
    "    kl = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "\n",
    "    # normalize by batch size for stability\n",
    "    return (recon + beta * kl) / x.size(0)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# END TODO 3.1\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# TODO 3.2: Set up the optimizer\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# END TODO 3.2\n",
    "# ----------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Start training VAE...\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# TODO 4.1: Implement the training loop\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    overall_loss = 0\n",
    "    overall_recon = 0\n",
    "    overall_kl = 0\n",
    "    \n",
    "    loop = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}\")\n",
    "    \n",
    "    for batch_idx, (x, _) in loop:\n",
    "        x = x.view(batch_size, x_dim).to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x_hat, mean, log_var = model(x)\n",
    "\n",
    "        # Compute separate components\n",
    "        recon = F.binary_cross_entropy(x_hat, x, reduction=\"sum\") / x.size(0)\n",
    "        kl = (-0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())) / x.size(0)\n",
    "        loss = recon + kl  # optionally multiply kl by beta if needed: recon + beta * kl\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        overall_loss += loss.item()\n",
    "        overall_recon += recon.item()\n",
    "        overall_kl += kl.item()\n",
    "        \n",
    "        # Update tqdm with current batch components\n",
    "        loop.set_postfix(loss=loss.item(), recon=recon.item(), kl=kl.item())\n",
    "    \n",
    "    avg_loss = overall_loss / len(train_loader)\n",
    "    avg_recon = overall_recon / len(train_loader)\n",
    "    avg_kl = overall_kl / len(train_loader)\n",
    "    \n",
    "    print(f\"\\tEpoch {epoch + 1} complete! \"\n",
    "          f\"Average Loss: {avg_loss:.4f}, Recon: {avg_recon:.4f}, KL: {avg_kl:.4f}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# END TODO 4.1\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print(\"Finish!!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AE4353",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
